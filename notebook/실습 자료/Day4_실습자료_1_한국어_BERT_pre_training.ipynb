{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day4_실습자료_1_한국어_BERT_pre_training.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2sLaWNa64xAz"},"source":["# 한국어 BERT 모델 학습\n","\n","> 작성자      \n","```\n","* 김성현 (bananaband657@gmail.com)  \n","김바다 (qkek983@gmail.com)\n","박상희 (parksanghee0103@gmail.com)  \n","이정우 (jungwoo.l2.rs@gmail.com)\n","```\n","[CC BY-NC-ND](https://creativecommons.org/licenses/by-nc-nd/2.0/kr/)"]},{"cell_type":"markdown","metadata":{"id":"ItbY5oL-V5DW"},"source":["## 학습 데이터 가져오기"]},{"cell_type":"code","metadata":{"id":"wYqha-sZ4KAS"},"source":["!mkdir my_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQ7EGqFiJ9EH"},"source":["실습을 위해서 아주 작은 wiki 코퍼스를 가져와보겠습니다 :-)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_z0pedAT44Gl","executionInfo":{"status":"ok","timestamp":1617977731072,"user_tz":-540,"elapsed":4728,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"795449de-8ae5-4e3f-8876-c202eba39600"},"source":["!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100   408    0   408    0     0    374      0 --:--:--  0:00:01 --:--:--   374\n","  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n","  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n","100 1323k  100 1323k    0     0   658k      0  0:00:02  0:00:02 --:--:--  658k\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rYXccFQeKB7F"},"source":["물론 한국어 wiki 전체 코퍼스도 공유드릴게요!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eq_xSrYBKFuv","executionInfo":{"status":"ok","timestamp":1617977742567,"user_tz":-540,"elapsed":10521,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"c94ed842-83a3-44dd-a385-790e1bed1206"},"source":["!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" -o my_data/wiki_20190620.txt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   408    0   408    0     0   2443      0 --:--:-- --:--:-- --:--:--  2457\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100  476M    0  476M    0     0  52.7M      0 --:--:--  0:00:09 --:--:-- 66.3M\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gI3vg0Q2KW6K"},"source":["이제 본격적으로 BERT 학습을 위한 준비를 해봅시다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id8FcYRa48Gc","executionInfo":{"status":"ok","timestamp":1617977749591,"user_tz":-540,"elapsed":13585,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"bd8005ac-60bf-4e7e-f74f-5ed4f418a62f"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.2MB 15.1MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 48.7MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n","\u001b[K     |████████████████████████████████| 870kB 73.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=7d90e7472d5f21fe95b06b71fc6c29023e39c402a9496e64d804530761483e11\n","  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nIEUFdT4WIqd"},"source":["## Tokenizer 만들기\n","\n","corpus를 이용하 wordPiece tokenizer를 만들어보겠습니다.   \n","이제는 익숙하시죠? :-)"]},{"cell_type":"code","metadata":{"id":"lFTgG7yv4_n7"},"source":["!mkdir wordPieceTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGFiNRpV5Ban","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617977751920,"user_tz":-540,"elapsed":2297,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"679458fc-5ed6-4bb3-c6b5-aedea982af5b"},"source":["from tokenizers import BertWordPieceTokenizer\n","\n","# Initialize an empty tokenizer\n","wp_tokenizer = BertWordPieceTokenizer(\n","    clean_text=True,   # [\"이순신\", \"##은\", \" \", \"조선\"] ->  [\"이순신\", \"##은\", \"조선\"]\n","    # if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n","    handle_chinese_chars=True,  # 한자는 모두 char 단위로 쪼게버립니다.\n","    strip_accents=False,    # True: [YehHamza] -> [Yep, Hamza]\n","    lowercase=False,    # Hello -> hello\n",")\n","\n","# And then train\n","wp_tokenizer.train(\n","    files=\"my_data/wiki_20190620_small.txt\",\n","    vocab_size=20000,   # vocab size 를 지정해줄 수 있습니다.\n","    min_frequency=2,\n","    show_progress=True,\n","    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n","    wordpieces_prefix=\"##\"\n",")\n","\n","# Save the files\n","wp_tokenizer.save_model(\"wordPieceTokenizer\", \"my_tokenizer\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['wordPieceTokenizer/my_tokenizer-vocab.txt']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ngd6Z3aJ57VR","executionInfo":{"status":"ok","timestamp":1617977751921,"user_tz":-540,"elapsed":2271,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"58ac0836-370d-4dec-c519-386c385ac295"},"source":["print(wp_tokenizer.get_vocab_size())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["20000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fnPe0_4a5C-a","executionInfo":{"status":"ok","timestamp":1617977830311,"user_tz":-540,"elapsed":912,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"ef49c840-7753-4d22-d20b-d9cc3c33702b"},"source":["text = \"이순신은 조선 중기의 무신이다.\"\n","tokenized_text = wp_tokenizer.encode(text)\n","print(tokenized_text.tokens)\n","print(tokenized_text.ids)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['이', '##순', '##신은', '조선', '중', '##기의', '무신', '##이다', '.']\n","[707, 1049, 7595, 1999, 755, 2604, 13160, 1894, 16]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XLE2ZQEp5GBq"},"source":["## BERT 학습"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lrihNAMK5EsC","executionInfo":{"status":"ok","timestamp":1617977834031,"user_tz":-540,"elapsed":2745,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"39460834-5144-476a-acd1-889d1725fb04"},"source":["import torch\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"zye2l6fP6igt"},"source":["from transformers import BertConfig, BertForPreTraining, BertTokenizerFast"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qnx2l-RVZOfk"},"source":["위 과정에서 생성한 tokenizer를 불러옵니다."]},{"cell_type":"code","metadata":{"id":"0Wtm3umi6geG"},"source":["tokenizer = BertTokenizerFast(\n","    vocab_file='/content/wordPieceTokenizer/my_tokenizer-vocab.txt',\n","    max_len=128,\n","    do_lower_case=False,\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvjurG9_bWND","executionInfo":{"status":"ok","timestamp":1617977837968,"user_tz":-540,"elapsed":1015,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"329ad565-88fc-4575-c007-375566a98e1e"},"source":["print(tokenizer.tokenize(\"뷁은 [MASK] 조선 중기의 무신이다.\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[UNK]', '[', 'M', '##AS', '##K', ']', '조선', '중', '##기의', '무신', '##이다', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gpmBH_F5XYxf","executionInfo":{"status":"ok","timestamp":1617977844054,"user_tz":-540,"elapsed":969,"user":{"displayName":"바나나인간","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhCM2sOGSCeogj8b2W7aPl7KKywidts5H45gy6vCA=s64","userId":"05069217733258421588"}},"outputId":"63b205dd-dcdd-4f92-95c7-d9a02d0dc686"},"source":["tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n","print(tokenizer.tokenize(\"이순신은 [MASK] 중기의 무신이다.\"))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xH7JzbMv5xBt","outputId":"6ca1d422-8b4f-4cc7-a488-bb5407f91561"},"source":["from transformers import BertConfig, BertForPreTraining\n","\n","config = BertConfig(    # https://huggingface.co/transformers/model_doc/bert.html#bertconfig\n","    vocab_size=20000,\n","    # hidden_size=512,\n","    # num_hidden_layers=12,    # layer num\n","    # num_attention_heads=8,    # transformer attention head number\n","    # intermediate_size=3072,   # transformer 내에 있는 feed-forward network의 dimension size\n","    # hidden_act=\"gelu\",\n","    # hidden_dropout_prob=0.1,\n","    # attention_probs_dropout_prob=0.1,\n","    max_position_embeddings=128,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정\n","    # type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)\n","    # pad_token_id=0,\n","    # position_embedding_type=\"absolute\"\n",")\n","\n","model = BertForPreTraining(config=config)\n","model.num_parameters()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["101720098"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"Sn7SnD0VjUqi"},"source":["from transformers import DataCollatorForLanguageModeling\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ka2mjYRuKzo"},"source":["import torch\n","from torch.utils.data.dataset import Dataset\n","from transformers.tokenization_utils import PreTrainedTokenizer\n","from typing import Dict, List, Optional\n","import os\n","import json\n","import os\n","import pickle\n","import random\n","import time\n","import warnings\n","\n","from filelock import FileLock\n","\n","from transformers.utils import logging\n","\n","logger = logging.get_logger(__name__)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KNGbscSSZzLv"},"source":["BERT의 학습에서 가장 중요한 요소 중 하나는, 다음 문장을 예측하는 것 입니다.   \n","아래 코드를 통해 다음 문장을 예측하기 위한 dataset을 구성합니다.   "]},{"cell_type":"code","metadata":{"id":"3ekdXKzlt2qO"},"source":["class TextDatasetForNextSentencePrediction(Dataset):\n","    \"\"\"\n","    This will be superseded by a framework-agnostic approach soon.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        tokenizer: PreTrainedTokenizer,\n","        file_path: str,\n","        block_size: int,\n","        overwrite_cache=False,\n","        short_seq_probability=0.1,\n","        nsp_probability=0.5,\n","    ):\n","        # 여기 부분은 학습 데이터를 caching하는 부분입니다 :-)\n","        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n","\n","        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n","        self.short_seq_probability = short_seq_probability\n","        self.nsp_probability = nsp_probability\n","\n","        directory, filename = os.path.split(file_path)\n","        cached_features_file = os.path.join(\n","            directory,\n","            \"cached_nsp_{}_{}_{}\".format(\n","                tokenizer.__class__.__name__,\n","                str(block_size),\n","                filename,\n","            ),\n","        )\n","\n","        self.tokenizer = tokenizer\n","\n","        lock_path = cached_features_file + \".lock\"\n","\n","        # Input file format:\n","        # (1) One sentence per line. These should ideally be actual sentences, not\n","        # entire paragraphs or arbitrary spans of text. (Because we use the\n","        # sentence boundaries for the \"next sentence prediction\" task).\n","        # (2) Blank lines between documents. Document boundaries are needed so\n","        # that the \"next sentence prediction\" task doesn't span between documents.\n","        #\n","        # Example:\n","        # I am very happy.\n","        # Here is the second sentence.\n","        #\n","        # A new document.\n","\n","        with FileLock(lock_path):\n","            if os.path.exists(cached_features_file) and not overwrite_cache:\n","                start = time.time()\n","                with open(cached_features_file, \"rb\") as handle:\n","                    self.examples = pickle.load(handle)\n","                logger.info(\n","                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n","                )\n","            else:\n","                logger.info(f\"Creating features from dataset file at {directory}\")\n","                # 여기서부터 본격적으로 dataset을 만듭니다.\n","                self.documents = [[]]\n","                with open(file_path, encoding=\"utf-8\") as f:\n","                    while True: # 일단 문장을 읽고\n","                        line = f.readline()\n","                        if not line:\n","                            break\n","                        line = line.strip()\n","\n","                        # 이중 띄어쓰기가 발견된다면, 나왔던 문장들을 모아 하나의 문서로 묶어버립니다.\n","                        # 즉, 문단 단위로 데이터를 저장합니다.\n","                        if not line and len(self.documents[-1]) != 0:\n","                            self.documents.append([])\n","                        tokens = tokenizer.tokenize(line)\n","                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n","                        if tokens:\n","                            self.documents[-1].append(tokens)\n","                # 이제 코퍼스 전체를 읽고, 문서 데이터를 생성했습니다! :-)\n","                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n","                self.examples = []\n","                # 본격적으로 학습을 위한 데이터로 변형시켜볼까요?\n","                for doc_index, document in enumerate(self.documents):\n","                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다.\n","\n","                start = time.time()\n","                with open(cached_features_file, \"wb\") as handle:\n","                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","                logger.info(\n","                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n","                )\n","\n","    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n","        \"\"\"Creates examples for a single document.\"\"\"\n","        # 문장의 앞, 뒤에 [CLS], [SEP] token이 부착되기 때문에, 내가 지정한 size에서 2 만큼 빼줍니다.\n","        # 예를 들어 128 token 만큼만 학습 가능한 model을 선언했다면, 학습 데이터로부터는 최대 126 token만 가져오게 됩니다.\n","        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n","\n","        # We *usually* want to fill up the entire sequence since we are padding\n","        # to `block_size` anyways, so short sequences are generally wasted\n","        # computation. However, we *sometimes*\n","        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n","        # sequences to minimize the mismatch between pretraining and fine-tuning.\n","        # The `target_seq_length` is just a rough target however, whereas\n","        # `block_size` is a hard limit.\n","\n","        # 여기가 재밌는 부분인데요!\n","        # 위에서 설명했듯이, 학습 데이터는 126 token(128-2)을 채워서 만들어지는게 목적입니다.\n","        # 하지만 나중에 BERT를 사용할 때, 126 token 이내의 짧은 문장을 테스트하는 경우도 분명 많을 것입니다 :-)\n","        # 그래서 short_seq_probability 만큼의 데이터에서는 2-126 사이의 random 값으로 학습 데이터를 만들게 됩니다.\n","        target_seq_length = max_num_tokens\n","        if random.random() < self.short_seq_probability:\n","            target_seq_length = random.randint(2, max_num_tokens)\n","\n","        current_chunk = []  # a buffer stored current working segments\n","        current_length = 0\n","        i = 0\n","\n","        # 데이터 구축의 단위는 document 입니다\n","        # 이 때, 무조건 문장_1[SEP]문장_2 이렇게 만들어지는 것이 아니라,\n","        # 126 token을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 만들어질 수 있습니다.\n","        while i < len(document):\n","            segment = document[i]\n","            current_chunk.append(segment)\n","            current_length += len(segment)\n","            if i == len(document) - 1 or current_length >= target_seq_length:\n","                if current_chunk:\n","                    # `a_end` is how many segments from `current_chunk` go into the `A`\n","                    # (first) sentence.\n","                    a_end = 1\n","                    # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n","                    if len(current_chunk) >= 2:\n","                        a_end = random.randint(1, len(current_chunk) - 1)\n","                    tokens_a = []\n","                    for j in range(a_end):\n","                        tokens_a.extend(current_chunk[j])\n","                    # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n","                    tokens_b = []\n","                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n","                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n","                        is_random_next = True\n","                        target_b_length = target_seq_length - len(tokens_a)\n","\n","                        # This should rarely go for more than one iteration for large\n","                        # corpora. However, just to be careful, we try to make sure that\n","                        # the random document is not the same as the document\n","                        # we're processing.\n","                        for _ in range(10):\n","                            random_document_index = random.randint(0, len(self.documents) - 1)\n","                            if random_document_index != doc_index:\n","                                break\n","                        # 여기서 랜덤하게 선택합니다 :-)\n","                        random_document = self.documents[random_document_index]\n","                        random_start = random.randint(0, len(random_document) - 1)\n","                        for j in range(random_start, len(random_document)):\n","                            tokens_b.extend(random_document[j])\n","                            if len(tokens_b) >= target_b_length:\n","                                break\n","                        # We didn't actually use these segments so we \"put them back\" so\n","                        # they don't go to waste.\n","                        num_unused_segments = len(current_chunk) - a_end\n","                        i -= num_unused_segments\n","                    # Actual next\n","                    else:\n","                        is_random_next = False\n","                        for j in range(a_end, len(current_chunk)):\n","                            tokens_b.extend(current_chunk[j])\n","\n","                    # 이제 126 token을 넘는다면 truncation을 해야합니다.\n","                    # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n","                    # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n","                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n","                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n","                        while True:\n","                            total_length = len(tokens_a) + len(tokens_b)\n","                            if total_length <= max_num_tokens:\n","                                break\n","                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n","                            assert len(trunc_tokens) >= 1\n","                            # We want to sometimes truncate from the front and sometimes from the\n","                            # back to add more randomness and avoid biases.\n","                            if random.random() < 0.5:\n","                                del trunc_tokens[0]\n","                            else:\n","                                trunc_tokens.pop()\n","\n","                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n","\n","                    assert len(tokens_a) >= 1\n","                    assert len(tokens_b) >= 1\n","\n","                    # add special tokens\n","                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n","                    # add token type ids, 0 for sentence a, 1 for sentence b\n","                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n","                    \n","                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n","                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n","                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n","                    example = {\n","                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n","                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n","                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n","                    }\n","\n","                    self.examples.append(example)\n","\n","                current_chunk = []\n","                current_length = 0\n","\n","            i += 1\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i):\n","        return self.examples[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cvlBlEmF52ed","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7d5b743-e364-4ff7-cc5b-c742fd448353"},"source":["dataset = TextDatasetForNextSentencePrediction(\n","    tokenizer=tokenizer,\n","    file_path='/content/my_data/wiki_20190620_small.txt',\n","    block_size=128,\n","    overwrite_cache=False,\n","    short_seq_probability=0.1,\n","    nsp_probability=0.5,\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (138 > 128). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Jt52m1cUbCIl"},"source":["이렇게 만들어진 학습 데이터를 확인해봅니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZGsKlvDQBBI","outputId":"d67ebf30-2c22-4d0c-def7-0881129764d9"},"source":["for example in dataset.examples[0:1]:\n","    print(example)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input_ids': tensor([    2,  4353,   640,     5,  5496,     5,  5503,  9437,  2490,  2427,\n","         2779,  1967,  5378,  3112,  1939,  2406,    16,  5496, 10309, 16248,\n","          552,  1042,   823,  1224,  1453,   931, 16497, 12286,  1017,  3666,\n","           16,  6529,  8933,  1090,  2676,  1905,    16,     3,  5425,  9486,\n","         1055,  2481,  4311,  2214,  1903,  3237,  4311,  4517,  1900,    16,\n","         7037,  3479,  5425, 16051,  1017,     5,    33,  1106, 16847,  1448,\n","         1104,  3246, 10477,  5387,  6323,  3079,     5,  2282, 10957,  1912,\n","           16,  2003,  6586,  5148,  1196,  1239,  5956,  1090,  4100,  1903,\n","         1941,  1197,  2762,  2928, 15350,   140,  1120,  1900,    16,  2687,\n","         2695,  5498,  1034,  2689, 11208,  1111, 17857, 17378,  4517,  1034,\n","         7995,   620,   576,  1886,    16,  2812,  2003,  5956,  3417, 11773,\n","         2686, 12375, 17610,  1006,  2046,  1898,    16,  5988, 17377,  4403,\n","         1006,  7512,  2025,    14,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1]), 'next_sentence_label': tensor(1)}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xdwJsZDdWpni"},"source":["[MASK]를 부착하는 data collator의 역할은 다음과 같습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpeY7jjMWsTW","outputId":"833cfa7d-27ce-4774-c65b-a791d86d5873"},"source":["print(data_collator(dataset.examples))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'input_ids': tensor([[    2,  4353,   640,  ...,  2025,    14,     3],\n","        [    2, 14350,  1884,  ...,  1564,  1071,     3],\n","        [    2,  2828,  4529,  ...,  1013,   742,     3],\n","        ...,\n","        [    2, 15882,  7342,  ...,     0,     0,     0],\n","        [    2,  1982,  3953,  ...,  9865,     4,     3],\n","        [    2,  6998,  1929,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 1, 1, 1],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([1, 1, 0,  ..., 1, 0, 0]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100],\n","        [-100, -100, -100,  ..., -100, -100, -100],\n","        [-100, -100, -100,  ..., -100, -100, -100],\n","        ...,\n","        [-100, -100, -100,  ..., -100, -100, -100],\n","        [-100, -100, -100,  ..., -100,   16, -100],\n","        [-100, -100, -100,  ..., -100, -100, -100]])}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMMUbKr_WvhG","outputId":"1dba99d1-b02b-4868-c4af-11a778d072ce"},"source":["print(data_collator(dataset.examples)['input_ids'][0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([    2,  4353,   640,     4,  5496,     5,  5503,  9437,  2490,     4,\n","         2779,     4,  5378,  3112,  1939,  2406,     4,  5496,     4, 16248,\n","          552,  1042,   823,  1224,  1453,     4, 16497, 12286,  1017,  3666,\n","            4,     4,  8933,  1090,  2676,     4,    16,     3,  5425,  9486,\n","         1055,  2481,  4311,     4,  1903,  3237,  4311,  4517,  1900,    16,\n","            4,  3479,  5425, 16051,  1017,     4,     4,  1106, 16847,  1448,\n","         1104,  3246, 10477,  5387,  6323,  3079,     5,  2282, 10957,  1912,\n","           16,     4,  6586,  5148,  1196,  1239,     4,  1090,  4100,  1903,\n","         1941,  1197,  2762,  2928, 15350,   140,  1120,     4,     4,     4,\n","         2695,  5498,  1034,  2689, 11208,  1111,     4, 17378,  4517,  1034,\n","         7995,   620,   576,  1886,    16,  2812,  2003,     4,  3417, 11773,\n","         2686, 12375, 17610,     4,     4,  1898,    16,  5879, 17377,  4403,\n","         1006,     4,     4,    14,     3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nWbI_dEJeguW"},"source":["collator 함수가 실행되면, 입력 문장에 [MASK] 가 부착됩니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":124},"id":"5lzsg6RIVIIN","outputId":"6364b746-97ec-4f5c-b73b-e6fe6e79229e"},"source":["tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'[CLS] 제임스 [MASK] \" [MASK] \" 카터 [MASK] [MASK] 민주당 출신 미국 39번째 대통령 이다. 지미 카터는 조지아주 섬터 기여를운 [MASK] 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. [SEP] 만은 카이저 [MASK] 보수주의를 지지하고 진보주의를 공격한다. [MASK] 국토 [MASK] 베를린에서 \" An Appeal [MASK] Reason \" 라는 연설을 한다. 그는 강하게 나치중심 사회주의 취 비난하고 운동 [MASK]들에 [MASK] 반대를 격력한다. 이것은 [MASK] 집필한 수많은 평론과 문학에서 나치를 공격한 것에서 알 수 있다. 동시에 그는 사회주의자들의 생각에 대해서 [MASK] 동정을 표현했다. 1933년 비효 집권 [MASK] 했을 당시율 [SEP]'"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"AEGfg7JL7KWJ"},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='model_output',\n","    overwrite_output_dir=True,\n","    num_train_epochs=10,\n","    per_gpu_train_batch_size=32,\n","    save_steps=1000,\n","    save_total_limit=2,\n","    logging_steps=100\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wwdMy08j7u-F","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"5697f8f8-ee9a-4359-c223-84aca6bb874f"},"source":["trainer.train() # wiki 전체 데이터로 학습 시, 1 epoch에 9시간 정도 소요됩니다!! "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n","Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [410/410 05:45, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>9.562900</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>9.081100</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>8.922200</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>8.796000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=410, training_loss=9.08109841695646, metrics={'train_runtime': 346.4642, 'train_samples_per_second': 1.183, 'total_flos': 1986593513940000.0, 'epoch': 10.0, 'init_mem_cpu_alloc_delta': 327414, 'init_mem_gpu_alloc_delta': 406882304, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 462912, 'train_mem_gpu_alloc_delta': 1224589312, 'train_mem_cpu_peaked_delta': 731420, 'train_mem_gpu_peaked_delta': 9485127168})"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"6F-dEL8TZ1eo"},"source":["trainer.save_model('.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BtuDt--Yb1Bx"},"source":["BERT 학습이 완료되었습니다! :-)   \n","아래 코드를 실행해서 [MASK] 테스트도 가능합니다.   "]},{"cell_type":"code","metadata":{"id":"rrKvxFAMVg9I"},"source":["from transformers import BertForMaskedLM, pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gCg6A9rMVsIY","outputId":"d0be4d88-48d6-4991-f1b0-68d00202b860"},"source":["my_model = BertForMaskedLM.from_pretrained('model_output')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at model_output were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1e1pqEqKWgGT","outputId":"606de7fa-5918-4cd9-effc-37725b7d1d3a"},"source":["tokenizer.tokenize('이순신은 [MASK] 중기의 무신이다.')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['이', '##순', '##신은', '[MASK]', '중', '##기의', '무신', '##이다', '.']"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"RVYg7-PmVzvH"},"source":["nlp_fill = pipeline('fill-mask', top_k=5, model=my_model, tokenizer=tokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IPn9azZoV7aC","outputId":"57cedb95-e89c-4709-e31c-cdda48a9fc95"},"source":["nlp_fill('이순신은 [MASK] 중기의 무신이다.')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.0225560050457716,\n","  'sequence': '[CLS] 이순신은, 중기의 무신이다. [SEP]',\n","  'token': 14,\n","  'token_str': ','},\n"," {'score': 0.02064059115946293,\n","  'sequence': '[CLS] 이순신은. 중기의 무신이다. [SEP]',\n","  'token': 16,\n","  'token_str': '.'},\n"," {'score': 0.008757401257753372,\n","  'sequence': '[CLS] 이순신은에 중기의 무신이다. [SEP]',\n","  'token': 1013,\n","  'token_str': '##에'},\n"," {'score': 0.006544862408190966,\n","  'sequence': \"[CLS] 이순신은'중기의 무신이다. [SEP]\",\n","  'token': 9,\n","  'token_str': \"'\"},\n"," {'score': 0.006350320298224688,\n","  'sequence': '[CLS] 이순신은 있다 중기의 무신이다. [SEP]',\n","  'token': 1886,\n","  'token_str': '있다'}]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kYenPbpZqhP","outputId":"da49eb2f-2d5a-417a-f1b3-311bc04b6812"},"source":["nlp_fill('[MASK]는 조선 중기의 무신이다.')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.025111474096775055,\n","  'sequence': '[CLS], 는 조선 중기의 무신이다. [SEP]',\n","  'token': 14,\n","  'token_str': ','},\n"," {'score': 0.022440314292907715,\n","  'sequence': '[CLS]. 는 조선 중기의 무신이다. [SEP]',\n","  'token': 16,\n","  'token_str': '.'},\n"," {'score': 0.007556694559752941,\n","  'sequence': '[CLS]의 는 조선 중기의 무신이다. [SEP]',\n","  'token': 1055,\n","  'token_str': '##의'},\n"," {'score': 0.006759335286915302,\n","  'sequence': '[CLS] 있다 는 조선 중기의 무신이다. [SEP]',\n","  'token': 1886,\n","  'token_str': '있다'},\n"," {'score': 0.006322021596133709,\n","  'sequence': '[CLS]에 는 조선 중기의 무신이다. [SEP]',\n","  'token': 1013,\n","  'token_str': '##에'}]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"1AvZG_qmQKBL"},"source":["끗!"]},{"cell_type":"code","metadata":{"id":"i9JiMJ3gaQUC"},"source":[""],"execution_count":null,"outputs":[]}]}