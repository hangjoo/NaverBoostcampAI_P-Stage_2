{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a49ef403-4986-46cf-a3ab-776ccc005fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import import_module\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import ElectraForSequenceClassification, ElectraConfig, AutoTokenizer, ElectraModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374d15d-ece6-4986-9b87-afb30abdde8c",
   "metadata": {},
   "source": [
    "# Electra Tokenizer, Config, Model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "300b242a-302f-43b8-a4d7-6c2ad58922d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"Electra\"\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_config = getattr(import_module(\"transformers\"), MODEL_TYPE + \"Config\").from_pretrained(MODEL_NAME)\n",
    "model = getattr(import_module(\"transformers\"), MODEL_TYPE + \"Model\").from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7540d67-b631-40a2-af99-1b4aca938259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db5d59a8-3784-46f9-8949-c3c6cc88b3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640538fb-5d93-4acd-ba9b-4434a3c9e6e8",
   "metadata": {},
   "source": [
    "# 학습 데이터 데이터 프레임 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f19ba7a-8255-4731-be32-fc062023f6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia-24896-25-30-33-19-21</td>\n",
       "      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n",
       "      <td>랜드로버</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>자동차</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>단체:제작</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia-12728-224-5-7-42-44</td>\n",
       "      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n",
       "      <td>민주당</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>27석</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>관계_없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wikipedia-28460-3-0-7-9-12</td>\n",
       "      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n",
       "      <td>유럽 축구 연맹</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>UEFA</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>단체:별칭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wikipedia-11479-37-24-26-3-5</td>\n",
       "      <td>용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...</td>\n",
       "      <td>강수일</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>공격수</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>인물:직업/직함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wikipedia-15581-6-0-2-32-40</td>\n",
       "      <td>람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...</td>\n",
       "      <td>람캄행</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>퍼쿤 씨 인트라팃</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>인물:부모님</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>wikipedia-5414-12-15-21-0-4</td>\n",
       "      <td>2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...</td>\n",
       "      <td>사우디아라비아</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>2002년</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>관계_없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>wikipedia-10384-4-12-14-0-1</td>\n",
       "      <td>일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...</td>\n",
       "      <td>토요타</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>일본</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>단체:본사_국가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>wikipedia-25913-6-8-10-93-106</td>\n",
       "      <td>방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...</td>\n",
       "      <td>방덕룡</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>선무원종공신(宣武原從功臣)</td>\n",
       "      <td>93</td>\n",
       "      <td>106</td>\n",
       "      <td>인물:직업/직함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>wikitree-12062-15-0-3-46-47</td>\n",
       "      <td>LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...</td>\n",
       "      <td>LG전자</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>북미</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>관계_없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>wikitree-21265-0-16-18-20-21</td>\n",
       "      <td>전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...</td>\n",
       "      <td>차영수</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>의원</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>인물:직업/직함</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0  \\\n",
       "0     wikipedia-24896-25-30-33-19-21   \n",
       "1      wikipedia-12728-224-5-7-42-44   \n",
       "2         wikipedia-28460-3-0-7-9-12   \n",
       "3       wikipedia-11479-37-24-26-3-5   \n",
       "4        wikipedia-15581-6-0-2-32-40   \n",
       "...                              ...   \n",
       "8995     wikipedia-5414-12-15-21-0-4   \n",
       "8996     wikipedia-10384-4-12-14-0-1   \n",
       "8997   wikipedia-25913-6-8-10-93-106   \n",
       "8998     wikitree-12062-15-0-3-46-47   \n",
       "8999    wikitree-21265-0-16-18-20-21   \n",
       "\n",
       "                                                      1         2   3   4  \\\n",
       "0     영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...      랜드로버  30  33   \n",
       "1     선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...       민주당   5   7   \n",
       "2     유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  유럽 축구 연맹   0   7   \n",
       "3     용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...       강수일  24  26   \n",
       "4     람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...       람캄행   0   2   \n",
       "...                                                 ...       ...  ..  ..   \n",
       "8995  2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...   사우디아라비아  15  21   \n",
       "8996  일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...       토요타  12  14   \n",
       "8997  방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...       방덕룡   8  10   \n",
       "8998  LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...      LG전자   0   3   \n",
       "8999  전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...       차영수  16  18   \n",
       "\n",
       "                   5   6    7         8  \n",
       "0                자동차  19   21     단체:제작  \n",
       "1                27석  42   44     관계_없음  \n",
       "2               UEFA   9   12     단체:별칭  \n",
       "3                공격수   3    5  인물:직업/직함  \n",
       "4          퍼쿤 씨 인트라팃  32   40    인물:부모님  \n",
       "...              ...  ..  ...       ...  \n",
       "8995           2002년   0    4     관계_없음  \n",
       "8996              일본   0    1  단체:본사_국가  \n",
       "8997  선무원종공신(宣武原從功臣)  93  106  인물:직업/직함  \n",
       "8998              북미  46   47     관계_없음  \n",
       "8999              의원  20   21  인물:직업/직함  \n",
       "\n",
       "[9000 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join(\"..\", \"data\", \"train\")\n",
    "data_df = pd.read_csv(os.path.join(data_path, \"train.tsv\"), sep=\"\\t\", header=None)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddc4c4-79a3-4f8d-a5db-05e448e3d456",
   "metadata": {},
   "source": [
    "# Tokenize 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b647404f-ad7f-45b8-834b-a75829a5ceaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : 이부는 원상과 헤어지게 되었으므로 어쩔 수 없이 원담(袁譚)에게 항복했고 평원(平原)에 부임하였다.\n",
      "Tokenized : 이부/##는/원상/##과/헤어지/##게/되/##었/##으므로/어쩔/수/없이/원/##담/(/袁/[UNK]/)/에게/항복/##했/##고/평원/(/平/原/)/에/부임/##하/##였/##다/.\n",
      "Encoded : [2, 29038, 4034, 21843, 4047, 17957, 4325, 2411, 4480, 15542, 9461, 2967, 6419, 3201, 4274, 12, 1652, 1, 13, 6220, 15158, 4398, 4219, 24046, 12, 845, 574, 13, 3130, 14372, 4279, 4737, 4176, 18, 3]\n"
     ]
    }
   ],
   "source": [
    "tokenize_sent = data_df.iloc[37, 1]\n",
    "\n",
    "print(f\"Original : {tokenize_sent}\")\n",
    "print(f\"Tokenized : {'/'.join(tokenizer.tokenize(tokenize_sent))}\")\n",
    "print(f\"Encoded : {tokenizer.encode(tokenize_sent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0acbfd3d-1957-4a30-855c-76a2d8e540a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"[SEP]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25d138e-b8c0-47db-8265-753dfa0b1f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7174276a-4409-4e6c-be93-1e914a404fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"[CLS]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c41cc9-1d0c-4712-9633-bafe0ca557bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "[UNK]\n",
      "[CLS]\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32562166-130d-4ecd-b161-57bc9534fc9f",
   "metadata": {},
   "source": [
    "Electra Tokenizer로 인코딩을 하면 자동으로 문장 처음에는 [CLS] 토큰이 끝에는 [SEP] 토큰이 생성되는 것을 확인할 수 있음.  \n",
    "그리고 인코딩된 정수형 인덱스에서 0은 패딩 토큰([PAD]), 1은 언노운 토큰([UNK]), 2는 클래스 토큰([CLS]), 3은 분리 토큰([SEP])임을 알 수 있음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e95a31b4-ff4a-473e-8edf-38dee359d667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', '##ll', '##o', 'Im', 'ha', '##ng', '##j', '##oo']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize([\"hello\", \"Im hangjoo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "529d9b0a-b022-4ef2-bc8f-cec24162b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = tokenizer(\n",
    "    text=data_df.iloc[:, 1].to_list(),\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=100,\n",
    "    add_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5e85521-9768-4e0a-8339-07c561f4a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[    2, 19604, 18268,  4008,     3, 31106,     5,     3,     0,     0],\n",
      "        [    2, 21866, 12904, 12557,     3, 21196, 12615,  4220, 16545,     3]])\n",
      "torch.Size([2, 10])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n",
      "torch.Size([2, 10])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokenized_sent.items():\n",
    "    print(k, v)\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4039f642-1f37-4624-bac9-e4681547bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3783,  0.1481, -0.2795,  ...,  0.2298, -0.3127,  0.7684],\n",
      "         [ 0.1961,  0.4683, -0.5439,  ...,  0.1308, -0.1579, -0.8483],\n",
      "         [ 0.2506, -0.0540, -0.5331,  ...,  0.1553, -0.6999,  0.2225],\n",
      "         ...,\n",
      "         [ 0.3780,  0.1540, -0.2798,  ...,  0.2266, -0.3079,  0.7678],\n",
      "         [ 0.0734,  0.1770,  0.0052,  ...,  0.2063,  0.4673, -0.7800],\n",
      "         [ 0.3186,  0.1875, -0.1589,  ...,  0.1055,  0.2299, -0.5941]],\n",
      "\n",
      "        [[ 0.1948,  0.0405, -0.2541,  ...,  0.2638, -0.3665,  0.7497],\n",
      "         [ 0.0683,  0.4253,  0.0120,  ..., -0.2512, -0.0936, -0.2599],\n",
      "         [-0.0121, -0.0328, -0.1212,  ...,  0.0304, -0.0629, -0.0347],\n",
      "         ...,\n",
      "         [-0.5421, -0.1155,  0.0861,  ..., -0.0772,  0.1519, -0.1864],\n",
      "         [-0.3997, -0.0418,  0.1718,  ..., -0.1247,  0.1347,  0.0721],\n",
      "         [ 0.1952,  0.0462, -0.2543,  ...,  0.2609, -0.3606,  0.7491]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "model = ElectraModel.from_pretrained(MODEL_NAME)\n",
    "output = model(**tokenized_sent)\n",
    "print(output.last_hidden_state)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10641069-4353-4f61-9ec2-ff95f9675ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0729,  1.5832, -0.1638,  ..., -0.4335,  1.1890,  1.0299],\n",
      "         [-0.4465, -0.1209, -0.7055,  ...,  0.5445,  0.6765,  1.4359],\n",
      "         [-0.2695, -0.1258,  0.4331,  ...,  0.2604,  0.6639,  0.7231],\n",
      "         ...,\n",
      "         [-0.7475,  0.5739, -0.1490,  ...,  0.2755,  0.5541,  0.4332],\n",
      "         [ 0.1801,  0.6906, -0.4432,  ..., -0.6756, -0.6663,  0.8265],\n",
      "         [-0.5208,  1.1501, -1.2500,  ..., -0.7293,  0.4956,  1.8243]],\n",
      "\n",
      "        [[ 0.0381,  0.7897, -0.7981,  ...,  0.5513,  1.1438,  1.3262],\n",
      "         [-1.4069, -0.1161, -0.2860,  ...,  1.7845,  0.3445,  1.0335],\n",
      "         [-1.6004,  1.4282, -0.2942,  ...,  0.6189,  1.7044,  1.5438],\n",
      "         ...,\n",
      "         [ 0.0999,  0.5018,  0.1397,  ...,  0.0052,  0.1692, -0.7591],\n",
      "         [-0.2231,  0.7950,  1.5741,  ...,  0.8118, -1.3511, -0.4199],\n",
      "         [-1.0749, -0.0117,  0.2787,  ..., -1.0062,  0.3903,  1.9678]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "model = ElectraModel(electra_config)\n",
    "output = model(**tokenized_sent)\n",
    "print(output.last_hidden_state)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6137e999-0d59-4c63-b878-60af83e565f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pickle as pkl\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer, token_max_len=256, max_label_count=300):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_max_len = 256\n",
    "\n",
    "        self.preprocessed_ = self.preprocessing(data_df, max_label_count)\n",
    "        self.tokenized_ = self.tokenizing()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenizer's input sentence would have a form like \"[CLS], entity_a's name, [SEP], entity_b's name, [SEP] sentence [SEP]\".\n",
    "        encoded = {k: v[idx] for k, v in self.tokenized_.items()}\n",
    "        label = torch.tensor(self.preprocessed_[\"labels\"][idx])\n",
    "\n",
    "        return encoded, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.preprocessed_[\"sentence\"].size\n",
    "\n",
    "    def preprocessing(self, data_df, max_label_count):\n",
    "        preprocessed_ = pd.DataFrame(columns=data_df.columns)\n",
    "\n",
    "        label_num = len(data_df[8].value_counts())\n",
    "        label_keys = [k for k in data_df[8].value_counts().keys()]\n",
    "\n",
    "        for i in range(label_num):\n",
    "            label_data_df = data_df.loc[data_df[8] == label_keys[i], :]\n",
    "            if len(label_data_df) >= max_label_count:\n",
    "                sample_df = label_data_df.sample(n=max_label_count)\n",
    "            else:\n",
    "                sample_df = label_data_df.sample(n=max_label_count, replace=True)\n",
    "            preprocessed_ = pd.concat([preprocessed_, sample_df], ignore_index=True)\n",
    "\n",
    "        with open(\"../data/label_type.pkl\", \"rb\") as f:\n",
    "            label_encoder = pkl.load(f)\n",
    "\n",
    "        label = []\n",
    "        for v in preprocessed_[8]:\n",
    "            if v == \"blind\":\n",
    "                label.append(100)\n",
    "            else:\n",
    "                label.append(label_encoder[v])\n",
    "\n",
    "        preprocessed_ = {\n",
    "            \"sentence\": preprocessed_[1].to_numpy(),\n",
    "            \"e1_name\": preprocessed_[2].to_numpy(),\n",
    "            \"e1_idx\": preprocessed_[[3, 4]].to_numpy(),\n",
    "            \"e2_name\": preprocessed_[5].to_numpy(),\n",
    "            \"e2_idx\": preprocessed_[[6, 7]].to_numpy(),\n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "        return preprocessed_\n",
    "\n",
    "    def tokenizing(self):\n",
    "        concat_entity = [e_a + \"[SEP]\" + e_b for e_a, e_b in zip(self.preprocessed_[\"e1_name\"], self.preprocessed_[\"e2_name\"])]\n",
    "        tokenized_sentences = self.tokenizer(\n",
    "            text=concat_entity,\n",
    "            text_pair=list(self.preprocessed_[\"sentence\"]),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.token_max_len,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c861c79c-5511-4790-9dcc-5f43540fa8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2, 24004,    21,  ...,     0,     0,     0],\n",
       "        [    2,  2720,     3,  ...,     0,     0,     0],\n",
       "        [    2,  3323,     3,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,  7694,  7303,  ...,     0,     0,     0],\n",
       "        [    2,  7694,  7303,  ...,     0,     0,     0],\n",
       "        [    2,  7694,  7303,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyDataset(data_df, tokenizer).tokenizing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12b125-86b6-4f64-8904-ef26e5a7a745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
