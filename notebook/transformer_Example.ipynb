{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49ef403-4986-46cf-a3ab-776ccc005fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import ElectraForSequenceClassification, ElectraConfig, AutoTokenizer, ElectraModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374d15d-ece6-4986-9b87-afb30abdde8c",
   "metadata": {},
   "source": [
    "# Electra Tokenizer, Config, Model 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300b242a-302f-43b8-a4d7-6c2ad58922d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"monologg/koelectra-small-v3-discriminator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "electra_config = ElectraConfig.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23c879c0-9511-4fc5-b7c6-04f57c99f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.2.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(electra_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb6d9939-d79e-4c09-9096-0775f184ee7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.2.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 35000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "electra_config.num_labels = 42\n",
    "print(electra_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1688e330-287f-4102-ac11-02b863b45c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# electra_config.vocab_size = 35002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2744f966-dd5c-4fac-8a2d-cad6155e6186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_config.return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5d59a8-3784-46f9-8949-c3c6cc88b3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraForSequenceClassification(\n",
      "  (electra): ElectraModel(\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(35000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ElectraClassificationHead(\n",
      "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=256, out_features=42, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification(electra_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640538fb-5d93-4acd-ba9b-4434a3c9e6e8",
   "metadata": {},
   "source": [
    "# 학습 데이터 데이터 프레임 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f19ba7a-8255-4731-be32-fc062023f6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia-24896-25-30-33-19-21</td>\n",
       "      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n",
       "      <td>랜드로버</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>자동차</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>단체:제작</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia-12728-224-5-7-42-44</td>\n",
       "      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n",
       "      <td>민주당</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>27석</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>관계_없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wikipedia-28460-3-0-7-9-12</td>\n",
       "      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n",
       "      <td>유럽 축구 연맹</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>UEFA</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>단체:별칭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wikipedia-11479-37-24-26-3-5</td>\n",
       "      <td>용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...</td>\n",
       "      <td>강수일</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>공격수</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>인물:직업/직함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wikipedia-15581-6-0-2-32-40</td>\n",
       "      <td>람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...</td>\n",
       "      <td>람캄행</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>퍼쿤 씨 인트라팃</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>인물:부모님</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>wikipedia-5414-12-15-21-0-4</td>\n",
       "      <td>2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...</td>\n",
       "      <td>사우디아라비아</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>2002년</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>관계_없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>wikipedia-10384-4-12-14-0-1</td>\n",
       "      <td>일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...</td>\n",
       "      <td>토요타</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>일본</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>단체:본사_국가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>wikipedia-25913-6-8-10-93-106</td>\n",
       "      <td>방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...</td>\n",
       "      <td>방덕룡</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>선무원종공신(宣武原從功臣)</td>\n",
       "      <td>93</td>\n",
       "      <td>106</td>\n",
       "      <td>인물:직업/직함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>wikitree-12062-15-0-3-46-47</td>\n",
       "      <td>LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...</td>\n",
       "      <td>LG전자</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>북미</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>관계_없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>wikitree-21265-0-16-18-20-21</td>\n",
       "      <td>전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...</td>\n",
       "      <td>차영수</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>의원</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>인물:직업/직함</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0  \\\n",
       "0     wikipedia-24896-25-30-33-19-21   \n",
       "1      wikipedia-12728-224-5-7-42-44   \n",
       "2         wikipedia-28460-3-0-7-9-12   \n",
       "3       wikipedia-11479-37-24-26-3-5   \n",
       "4        wikipedia-15581-6-0-2-32-40   \n",
       "...                              ...   \n",
       "8995     wikipedia-5414-12-15-21-0-4   \n",
       "8996     wikipedia-10384-4-12-14-0-1   \n",
       "8997   wikipedia-25913-6-8-10-93-106   \n",
       "8998     wikitree-12062-15-0-3-46-47   \n",
       "8999    wikitree-21265-0-16-18-20-21   \n",
       "\n",
       "                                                      1         2   3   4  \\\n",
       "0     영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...      랜드로버  30  33   \n",
       "1     선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...       민주당   5   7   \n",
       "2     유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  유럽 축구 연맹   0   7   \n",
       "3     용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...       강수일  24  26   \n",
       "4     람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...       람캄행   0   2   \n",
       "...                                                 ...       ...  ..  ..   \n",
       "8995  2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...   사우디아라비아  15  21   \n",
       "8996  일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...       토요타  12  14   \n",
       "8997  방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...       방덕룡   8  10   \n",
       "8998  LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...      LG전자   0   3   \n",
       "8999  전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...       차영수  16  18   \n",
       "\n",
       "                   5   6    7         8  \n",
       "0                자동차  19   21     단체:제작  \n",
       "1                27석  42   44     관계_없음  \n",
       "2               UEFA   9   12     단체:별칭  \n",
       "3                공격수   3    5  인물:직업/직함  \n",
       "4          퍼쿤 씨 인트라팃  32   40    인물:부모님  \n",
       "...              ...  ..  ...       ...  \n",
       "8995           2002년   0    4     관계_없음  \n",
       "8996              일본   0    1  단체:본사_국가  \n",
       "8997  선무원종공신(宣武原從功臣)  93  106  인물:직업/직함  \n",
       "8998              북미  46   47     관계_없음  \n",
       "8999              의원  20   21  인물:직업/직함  \n",
       "\n",
       "[9000 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.path.join(\"..\", \"data\", \"train\")\n",
    "data_df = pd.read_csv(os.path.join(data_path, \"train.tsv\"), sep=\"\\t\", header=None)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ddc4c4-79a3-4f8d-a5db-05e448e3d456",
   "metadata": {},
   "source": [
    "# Tokenize 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b647404f-ad7f-45b8-834b-a75829a5ceaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : 이부는 원상과 헤어지게 되었으므로 어쩔 수 없이 원담(袁譚)에게 항복했고 평원(平原)에 부임하였다.\n",
      "Tokenized : 이부/##는/원상/##과/헤어지/##게/되/##었/##으므로/어쩔/수/없이/원/##담/(/袁/[UNK]/)/에게/항복/##했/##고/평원/(/平/原/)/에/부임/##하/##였/##다/.\n",
      "Encoded : [2, 29038, 4034, 21843, 4047, 17957, 4325, 2411, 4480, 15542, 9461, 2967, 6419, 3201, 4274, 12, 1652, 1, 13, 6220, 15158, 4398, 4219, 24046, 12, 845, 574, 13, 3130, 14372, 4279, 4737, 4176, 18, 3]\n"
     ]
    }
   ],
   "source": [
    "tokenize_sent = data_df.iloc[37, 1]\n",
    "\n",
    "print(f\"Original : {tokenize_sent}\")\n",
    "print(f\"Tokenized : {'/'.join(tokenizer.tokenize(tokenize_sent))}\")\n",
    "print(f\"Encoded : {tokenizer.encode(tokenize_sent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0acbfd3d-1957-4a30-855c-76a2d8e540a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"[SEP]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a25d138e-b8c0-47db-8265-753dfa0b1f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7174276a-4409-4e6c-be93-1e914a404fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"[CLS]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c41cc9-1d0c-4712-9633-bafe0ca557bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "[UNK]\n",
      "[CLS]\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(tokenizer.decode(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32562166-130d-4ecd-b161-57bc9534fc9f",
   "metadata": {},
   "source": [
    "Electra Tokenizer로 인코딩을 하면 자동으로 문장 처음에는 [CLS] 토큰이 끝에는 [SEP] 토큰이 생성되는 것을 확인할 수 있음.  \n",
    "그리고 인코딩된 정수형 인덱스에서 0은 패딩 토큰([PAD]), 1은 언노운 토큰([UNK]), 2는 클래스 토큰([CLS]), 3은 분리 토큰([SEP])임을 알 수 있음!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "529d9b0a-b022-4ef2-bc8f-cec24162b4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = tokenizer(\n",
    "    text=[\"hello\", \"My name is\"],\n",
    "    text_pair=[\"world!\", \"hangjoo\"],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=100,\n",
    "    add_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e85521-9768-4e0a-8339-07c561f4a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[    2, 19604, 18268,  4008,     3, 31106,     5,     3,     0,     0],\n",
      "        [    2, 21866, 12904, 12557,     3, 21196, 12615,  4220, 16545,     3]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokenized_sent.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4039f642-1f37-4624-bac9-e4681547bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3783,  0.1481, -0.2795,  ...,  0.2298, -0.3127,  0.7684],\n",
      "         [ 0.1961,  0.4683, -0.5439,  ...,  0.1308, -0.1579, -0.8483],\n",
      "         [ 0.2506, -0.0540, -0.5331,  ...,  0.1553, -0.6999,  0.2225],\n",
      "         ...,\n",
      "         [ 0.3780,  0.1540, -0.2798,  ...,  0.2266, -0.3079,  0.7678],\n",
      "         [ 0.0734,  0.1770,  0.0052,  ...,  0.2063,  0.4673, -0.7800],\n",
      "         [ 0.3186,  0.1875, -0.1589,  ...,  0.1055,  0.2299, -0.5941]],\n",
      "\n",
      "        [[ 0.1948,  0.0405, -0.2541,  ...,  0.2638, -0.3665,  0.7497],\n",
      "         [ 0.0683,  0.4253,  0.0120,  ..., -0.2512, -0.0936, -0.2599],\n",
      "         [-0.0121, -0.0328, -0.1212,  ...,  0.0304, -0.0629, -0.0347],\n",
      "         ...,\n",
      "         [-0.5421, -0.1155,  0.0861,  ..., -0.0772,  0.1519, -0.1864],\n",
      "         [-0.3997, -0.0418,  0.1718,  ..., -0.1247,  0.1347,  0.0721],\n",
      "         [ 0.1952,  0.0462, -0.2543,  ...,  0.2609, -0.3606,  0.7491]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "model = ElectraModel.from_pretrained(MODEL_NAME)\n",
    "output = model(**tokenized_sent)\n",
    "print(output.last_hidden_state)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10641069-4353-4f61-9ec2-ff95f9675ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0729,  1.5832, -0.1638,  ..., -0.4335,  1.1890,  1.0299],\n",
      "         [-0.4465, -0.1209, -0.7055,  ...,  0.5445,  0.6765,  1.4359],\n",
      "         [-0.2695, -0.1258,  0.4331,  ...,  0.2604,  0.6639,  0.7231],\n",
      "         ...,\n",
      "         [-0.7475,  0.5739, -0.1490,  ...,  0.2755,  0.5541,  0.4332],\n",
      "         [ 0.1801,  0.6906, -0.4432,  ..., -0.6756, -0.6663,  0.8265],\n",
      "         [-0.5208,  1.1501, -1.2500,  ..., -0.7293,  0.4956,  1.8243]],\n",
      "\n",
      "        [[ 0.0381,  0.7897, -0.7981,  ...,  0.5513,  1.1438,  1.3262],\n",
      "         [-1.4069, -0.1161, -0.2860,  ...,  1.7845,  0.3445,  1.0335],\n",
      "         [-1.6004,  1.4282, -0.2942,  ...,  0.6189,  1.7044,  1.5438],\n",
      "         ...,\n",
      "         [ 0.0999,  0.5018,  0.1397,  ...,  0.0052,  0.1692, -0.7591],\n",
      "         [-0.2231,  0.7950,  1.5741,  ...,  0.8118, -1.3511, -0.4199],\n",
      "         [-1.0749, -0.0117,  0.2787,  ..., -1.0062,  0.3903,  1.9678]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "model = ElectraModel(electra_config)\n",
    "output = model(**tokenized_sent)\n",
    "print(output.last_hidden_state)\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137e999-0d59-4c63-b878-60af83e565f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
